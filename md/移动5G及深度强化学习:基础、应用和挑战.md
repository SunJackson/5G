# 简介:5G及更高领域的动态挑战

## 摘要

未来一代无线网络，即5G及更高版本，必须适应移动数据流量的激增，并支持具有各种服务和应用的高密度移动用户。 同时，网络变得越来越密集，异构，
分散，并且具有特殊的特征，涉及众多不同的网络实体。 这样，需要在服务中实现不同的目标，例如高吞吐量和低等待时间，并且必须相应地设计和优化资源分配。
然而，由于无线网络环境中固有存在的动态性和不确定性，需要完整和完美的系统知识的传统服务和资源管理方法变得低效甚至不适用。 受到机器学习在解决复杂
控制决策问题方面取得成功的启发，在本文中，我们将重点放在基于深度强化学习的方法上，这些方法允许网络知识学习和构建有关网络的知识，从而在本地独立
地做出最佳决策。 我们首先介绍深度加固学习的概述和基本概念。 接下来，我们回顾一些相关的工作，这些工作可以利用强化学习来解决5Gnetworks中的不同问
题。 最后，我们提出了深度加固学习在5G网络切片优化中的应用。 数值结果表明，与基线解相比，该方法实现了更好的性能。

### 索引术语 

移动5G，深度强化学习，智能资源管理，切片。

## 引言

智能移动设备和通信技术的普及推动了无线数据服务的迅猛增长，引发了对未来一代无线网络的调查，即5G及以上的网络。预计5G网络将支持各种应用。 多种要求，包括更高的峰值和用户数据速率，更短的延迟，增强的系统容量，更高的能源效率等。 为了实现这一目标，已经提出了一系列新兴无线技术，例如大规模MIMO（多输入多输出）和毫米波（mmWave）通信。 利用massMIMO，每个基站（BS）可以同时向多个用户设备（UE）发送高速数据流。或者，通过mmWave通信，BS可以有效地利用高频频谱来克服带宽不足并为UE提供更多的可用带宽。

除了物理网络的进步，即大规模的MIMO和mmWave之外，5G及以上的另一个主要驱动因素是网络软件化。 它可以在动态网络条件和服务需求下为移动业务管理提供更大的灵活性。 同时，通过上述支持大量UE的技术，5G网络本质上变得越来越异构和分散。 还涉及多个网络实体，例如，具有不同类型的BS和具有不同QoS要求的UE。 需要针对不同目标确定网络实体的优化决策，例如数据速率最大化，网络延迟和能量消耗最小化。 然而，在存在各种各样的服务要求以及移动5G网络环境中的固有动态和不确定性（例如快速时变无线信道和不断改变网络拓扑）的情况下实现优化资源和服务管理具有挑战性[ 1]。

在这种时变和不可预测的网络环境中，强化学习（RL）已被证明是解决实时动态决策问题的可行工具[2]。 一方面，RL在制定决策时自然而然地采用了远见卓识的系统演化，而不是对目前的优势进行近视优化，这对于时变5G网络来说是必不可少的。 另一方面，即使没有最新信息，RL也可以通过先前决策的奖励反馈更新决策政策以达到最佳系统效果，即使没有最新的信息[2]。 因此，基于RL的方法可以成为解决移动网络中资源和服务管理问题的一种选择。 然而，传统的RL算法，如Q Learning，收敛速度慢，尤其是问题的状态空间和动作空间很大。此外，算法必须存储每个状态的完整表格，例如Q-Value对。 这些表太大而无法在移动设备中维护。在这方面，RL经常导致性能不佳。 作为人工智能的支柱，深度强化学习（DRL）是RL与深度学习的结合，有望提出克服RL的局限性.DRL在自然语言处理和机器人技术等许多应用中已经取得了巨大的成功[3]。

由于DRL在处理大规模和动态系统方面具有巨大潜力，我们探索基于DRL的5G及以上方法，以提高网络性能。 在InDRL中，使用称为DeepQNetwork（DQN）的深度神经网络来加速学习过程，并减少存储模型参数所需的内存，使其非常适合具有有限资源的移动设备。 因此，使用DRL有望在复杂，密集和异构的移动5G环境下提供移动服务和网络管理和控制问题。

在本文中，我们首先介绍DRL的基本概念，然后回顾一些最近将DRL应用解决移动5G及以后出现的问题，如功率控制，卸载和边缘缓存。 接下来，我们介绍了基于DRL的网络切片优化方案，并表明该方案与其他方法相比具有最佳性能。 最后，我们将结合我们的工作并概述未来的重要研究方向。

## 深度强化学习概述

在本节中，我们介绍了强化学习的基础，然后讨论了从强化学习发展到深层强化学习的原因。

### 强化学习

作为重要的机器学习技术之一，强化学习（RL）能够在没有系统和环境的先验知识的情况下优化智能体的决策。 智能体在某个系统状态下执行操作，并观察来自环境的相应响应。 智能体要么接受采取正确行动的奖励，要么接受采取不良行动的惩罚。 然后，智能体在做出决定时采用试错法搜索可能的最佳状态-动作对（称为策略）。 鼓励智能体采取一系列行动，以最大化基于过去知识的智能体的长期奖励，即强化。

Q Learning 算法是最着名的无模型RL算法之一，用于计算最大化长期奖励的最优策略。 引入奖励函数以将状态-动作对映射到预期的累积奖励（即，Q值），以便智能体估计并且响应于不同的系统状态来确定最佳动作。通过记录使Q值最大化的所有动作，智能体获得最佳状态-动作对列表，定义为最优策略。

强化学习是解决移动通信系统中的资源管理和其他优化问题的有前途的工具，具有服务和资源可用性的时间变化和随机性，以及系统参数和状态。 为了优化系统服务和资源，网络实体的计算并更新用于最佳动作的Q函数。 由于移动通信系统中的不确定性以及智能体的短视，当在相同的系统状态下采取相同的动作时，代理获得的奖励可以是不同的。 在这种情况下，Q函数需要以更新的方式进行更新，直到以适当设置的更新速率（即学习速率）收敛。 Q函数更新过程被定义为训练。

## 从强化学习到深度强化学习

Q Learning需要智能体维护和更新所有状态-动作对的一组值。然而，下一代无线网络预计将是大规模，高度异构和分散的。 因此，系统状态的可能值的数量变得难以处理。 此外，由于系统组件和环境参数的高度多样性和不确定性，可能存在隐藏的系统状态甚至无限数量的系统状态。 在这种情况下，计算和维护去全部Q值的成本实际上变得不可行。 这种情况被称为维度灾难。

为了解决这个问题，Deep Reinforcement Learning（DRL）将RL与深度学习技术相结合。 采用Deep Q Network（DQN）的Deep Q Learning是典型的DRL模型之一，它将深度神经网络应用为Q函数的近似（见图1）。 注意，图1中的向前和状态可以基于5G中的不同系统目标来定义，例如吞吐量最大化或输出概率最小化，以及功耗或信道增益。 与RL中的Q函数类似，DQN将当前观察到的系统状态作为输入。 所有输入状态都转移到具有特定权重因子的不同神经网络层。 最后，DQN针对所有可能的动作输出一组Q值。 DQN中的学习目标是从历史数据（即历史Q值，动作和状态转换）中训练和找到最可行的权重因子。 对于具有多层感知器作为底层神经网络的DQN，计算Q值和动作的复杂性是线性的。 此外，DQN的输入数量仅由所有状态的类型决定。 对于每个输入，即使当所有值的数量达到无穷大时，状态的各种值也可以在例如不同的信道状态中传递，而不改变DQN的结构。 如果Q-函数是由神经网络构建的，则RL和DRL之间的主要区别在于以下两个方面：

1. 在RL中，获取的样本用于训练神经网络的参数，而DRL随机选择从内存池中的样本来训练DQN的参数。 
2. DRL每隔几步更新一次权重，以减少目标估计Q值之间的相关性，从而稳定DRL的学习过程。

与RL相比，DRL显著降低了模型的复杂性，特别是当它用于解决未来通信系统中的各种问题时。 此外，DRL在提取系统功能方面优于RL，以预测优化的奖励和行动。 虽然RL只考虑并更新当前的奖励和系统状态转换，但DRL存储并采集历史奖励，行动和状态转换到小型机[3]。 然后，DRL使用存储的历史数据来训练深度网络权重因子。 因此，在系统和环境知识的帮助下，可以加速培训过程。详细信息可以在[4]中找到。 此外，云计算服务和移动边缘计算设备可以集成到即将到来的5G及更高（6G）网络中。 移动用户和网络运营商将通过利用丰富的计算能力来解决资源管理和网络优化，从而受益于DRL。 例如，训练过程和历史数据存储可以分别在具有GPU和大容量存储空间的边缘设备处执行。

在下一节中，我们将回顾一些最近将DRL应用于移动5G及以上网络的工作。

## 深度强化学习在未来移动网络中的应用

在文献中，大多数优化问题的infuture生成网络已经通过集中优化方法得以解决。 但是，这些方法对网络条件下的完整信息要求做出了假设。 随着移动网络环境越来越难以预测，这些假设变得不切实际。 相反，DRL方法并未对目标系统做出强烈的假设，因此成为移动5G及更高版本中出现的不同问题的实用技术。 在下文中，我们回顾了一些重要的工作，它们利用DRL来解决5G网络中的不同问题。

### 功率控制和电源管理

5G网络中的干扰变得更具挑战性，因为日益异构和密集的环境使得传统的小区间干扰协调将变得低效甚至不可行。 发射机可以降低发射功率以减轻干扰。但是，数据速率可能会受到不利影响。 因此，干扰传导被视为移动网络的功率控制优化问题。 网络设备的电源管理（例如，打开或关闭）用于改善能量节省，因此降低了成本和碳足迹。 DRL允许网络实体建立关于网络的知识并且使其对功率控制和功率管理做出最佳决策。

#### 蜂窝网络中的功率控制

考虑到动态5G系统的不确定性，[5]的作者利用DRL开发了一种分布式动态功率分配方案。 在该方案中，每个发射机充当智能体，并且所有智能体同步并同时采取其行动。 在采取行动之前，智能体能够观察当前决策周期中邻居的过去行为所产生的影响，基于此，可以估计当前行为对邻居的未来行为的影响。 通过使用DRL，每个代理确定最大化预期未来奖励的策略。与传统的集中优化方案不同，所提出的DRL方案的计算复杂性不依赖于网络规模，当它应用于具有大覆盖区域的5G网络时实现高可扩展性。

#### 超密集网络中的电源管理

由于小型基站（SBS）的密集部署，Ultra Dense Networks（UDN）面临高功耗。 因此，动态关闭SBS是提高能效的有效解决方案。 在[6]中，作者将能量收集的SBSs ON / OFF问题表达为动态优化问题。 考虑到能量收费，CSI和交通到达的不确定性，DRL被引入以学习用于提高能效的SBS的自开/关模式的策略。 结果表明，基于DRL的ON / OFF调度方案实现了比通过Q Learning实现的更高的能量效率。

#### mmWave通信中的功率控制

随着5G中mmWave通信的部署，非视距（NLOS）传输是一个关键问题。 [7]中的作者提出了动态传输功率控制方案，以提高NLOS传输性能。 目标是在传输功率和QoS要求的约束下最大化5G网络中所有UE实现的总数据速率。 首先，卷积神经网络离线训练以估计Q函数。 然后，DRL在线执行以进行UE关联和功率分配。

### 计算卸载和边缘缓存

随着计算对于支持新兴移动应用程序和服务变得越来越重要，移动5G及其他设计将通过在移动网络边缘部署计算资源和缓存功能来设计。 这可以显着提高需要密集计算和低延迟的应用的能效和QoS。计算卸载和边缘缓存的研究由于异构用户之间的固有耦合，QoS配置，移动模式和无线电，动态系统通常涉及复杂的系统分析。 因此，基于学习的方法（如DRL）成为管理巨大的状态空间和优化变量的可行解决方案。

#### 移动边缘计算卸载



## 5G 与深度强化学习结合

- 功率控制和电源管理
    - 蜂窝网络中的功率控制
    - 超密集网络中的功率管理
    - mmWave通信中的功率控制
- 计算转移和边缘缓存
    - 移动边计算转移
    - 边缘缓存
    - 联合边缘计算和缓存
- 智能交通
- 网络切片


## 网络切片

- 基于DRL的网络切片方法